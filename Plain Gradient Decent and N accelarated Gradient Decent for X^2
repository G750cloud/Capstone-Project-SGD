def grad(x):
    return 2*x
'''
Plain Gradient Decent g(x) = x^2

'''
def plain_gd(grad, cur_x = 0.1, learning_rate = 0.01, precision = 0.0001, max_iters = 10000):
    for i in range(max_iters):
        grad_cur = grad(cur_x)
        if abs(grad_cur) < precision:
            break
        cur_x = cur_x - grad_cur * learning_rate
        print("第",i,"这次迭代：x值为", cur_x)
    print("局部最小值x是：", cur_x)
    return cur_x

plain_gd(grad, cur_x = 10, learning_rate = 0.02, precision = 0.0001, max_iters = 10000)

'''
Nesterov's accelarated Gradient Decent g(x) = x^2

'''

def N_gd(grad, cur_x1 = 0.2, cur_x2 = 0.1, learning_rate = 0.01, precision = 0.0001, max_iters = 10000):
    for i in range(max_iters):
        y = cur_x2+(i-2)/(i+1)*(cur_x2-cur_x1)      
        grad_cur = grad(y)
        if abs(grad_cur) < precision:
            break
        cur_x1 = cur_x2 
        cur_x2 = y - grad_cur * learning_rate
        print("第",i,"这次迭代：x值为", cur_x2)
    print("局部最小值x是：", cur_x2)
    return cur_x2

N_gd(grad, cur_x1 = 10, cur_x2 = 9.9, learning_rate = 0.02, precision = 0.0001, max_iters = 10000)
